{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROCKY V\n",
    "By Simen Svenkerud\n",
    "\n",
    "Version: 1.0\n",
    "Date: 2019.01.08\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "### Purpose\n",
    "This system has been build with the purpose of taking output data from the EFA NFS-ROW private equity system and: \n",
    "* Calculate the components of ROCKY V\n",
    "* Visualise timeseries by Sector for each of the components\n",
    "* Export data ready for further preparation into the wider EFA national acounts.\n",
    "\n",
    "### Descriptions\n",
    "The system is build primarily using PySpark however visualisation is performed in Matplotlib.\n",
    "This document has been generalised and encoded variables blanked out, cpompared to the active system within the VDI. \n",
    "Functionality remains the same, but the inpt hase been parameterised for generalisability.\n",
    "\n",
    "Further the visualisation section of this document takes advantage of the cell based nature of Notebooks to make visualisation more readable code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import window\n",
    "from pyspark.sql.functions import broadcast\n",
    "import pyspark.functions as F\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected spark session is relatively small, compared with others in the NFS-ROW system. There is a slightly extendended Memory alocation, due to the relative number of parameters that need to be calculated in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ROKY_V')\\\n",
    "                    .config('spark.executer.memory', '12g')\\\n",
    "                    .config('spark.executer.cores', 6)\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the base variables\n",
    "Below you set the variables that define the location of input data and loaction for output within the Hadoop cluster.\n",
    "\n",
    "Data_Start is a filter that limits in time when you want ROCKY V to be calculated from.\n",
    "FA_Instrument_Select as the base data come with all Financial acounts instruments included, this option alows you to filter down to a single instrument i.e. 'F511F for Private equity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Import_Location = ''\n",
    "Output = ''\n",
    "\n",
    "Data_Start = ''\n",
    "FA_Instrument_Select = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Column parameters\n",
    "In the section below enter the column names used in the data set itself. \n",
    "In the Active system this cell does not yet exist, and input variables are set by hardcoding. In a future itteration of the system this could be changed to include the paramaterisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FA_Instrument= ''\n",
    "ID = ''\n",
    "Quarter = ''\n",
    "Sector = ''\n",
    "Name = ''\n",
    "Shares_Outstanding = ''\n",
    "Price = ''\n",
    "Exchange_Rate = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load financial data set to be processed\n",
    "Load the data as a parquet table from the pre-specified Hadoop location, and reduce the nuber of columns and rows, to only the relevant for this analysis. This reduced the size of the data and improoves processing times. Data is repatitioned based on ID (Security_ID) as this creates individual timesereis for each security over which calculations are performed, improving processing time and limiting risk of errors.\n",
    "\n",
    "delta is a window used for the creation of lagged collumns, i.e. retain the value of the previouse quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data = spark.read.format('Parquet')\\\n",
    "                 .option('header', 'true')\\\n",
    "                 .option('inferSchema', 'true')\\\n",
    "                 .load(Import_Location)\\\n",
    "                 .filter(F.col(FA_Instrument) == FA_Instrument_Select)\\\n",
    "                 .repartition(F.col(ID))\\\n",
    "                 .select(F.col(ID),\n",
    "                         F.col(Quarter),\n",
    "                         F.col(Sector),\n",
    "                         F.col(Name),\n",
    "                         F.col(Shares_Outstanding),\n",
    "                         F.col(Price),\n",
    "                         F.col(Exchange_Rate)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta = Window.partitionedBy(F.col(ID)).orderBy(F.col(Quarter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing of the data\n",
    "In the first step timeseries are created and the columns which need to be lagged are created. NA's are replaced with 0 so that when change from t1 to t2 is calculates the no change value is 0 instead of 'nan' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = Data.orderBy(F.col(Quarter))\\\n",
    "            .select('*', F.lag(Shares_Outstanding).over(delta).alias('Shares_Outstanding_Lag'),\n",
    "                         F.lag(Price).over(delta).alias('Price_Lag'),\n",
    "                         F.lag(Exchange_Rate).over(delta).alias('Exchange_Rate_Lag'))\\\n",
    "            .fillna({Shares_Outstanding : 0.0})\\\n",
    "            .fillna({'Shares_Outstanding_Lag' : 0.0})\\\n",
    "            .fillna({'Price_Lag' : 0.0})\\\n",
    "            .fillna({'Exchange_Rate_Lag' : 0.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change and midpoint data is calculated based on the raw and lagged columns. Balance and Balance lag is calulated after adding price to the by number balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df1.withColumn('Net_Transactions', F.col(Shares_Outstanding) - F.col('Shares_Outstanding_Lag'))\\\n",
    "         .withColumn('Balance', F.col(Shares_Outstanding) * F.col(Price))\\\n",
    "         .withColumn('Delta_Price', F.col(Price) - F.col('Price_Lag'))\\\n",
    "         .withColumn('Delta_Exchange_rate', F.col(Exchange_Rate) - F.col('Exchange_Rate_Lag'))\\\n",
    "         .withColumn('Mean_price', ((F.col(Price) + F.col('Price_Lag'))/2)\\\n",
    "         .withColumn('Mean_Exchange_rate', ((F.col(Exchange_Rate) + F.col('Exchange_Rate_Lag'))/2)\\\n",
    "         .select('*', F.lag('Balance').over(delta).alias('Balance_Lag'))\\\n",
    "         .fillna({'Balance_Lag' : 0.0 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the ROCKY V elements are calculated as well as the Other volume Change and percentage Other Volume Change.\n",
    "\n",
    "* Transaction_Value =  The monitary value change due to the sale, or reaquiring of shares.\n",
    "* Market_FX_Effect = The effect of change in currency exchange controlled for change in number of shares and price\n",
    "* Market_Price_Effect = The effect of change in price controlled for change in number of shares and currency exchange rate\n",
    "* Other_Changes_Volumne = Any changes in value that cannot be explained by changes in number of shares, FX rate, or price.\n",
    "* pct_OVC = The percentage of total value the OVC constitutes.\n",
    "\n",
    "Note: Due to the current nature of data form TR we expect OVC's to be ~0, however due to how Spark handles large integers and calculation with integers the value might not be == 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.withColumn('Transactions_Value', F.col(Net_Transactions) * F.col('Mean_price'))\\\n",
    "         .withColumn('Market_FX_Effect', (((F.col(Exchange_Rate)/F.col('Exchange_Rate_Lag'))*F.col('Balance_Lag'))+((F.col(Exchange_Rate)/F.col('Mean_Exchange_rate'))*F.col('Transaction_Value'))-F.col('Balance_Lag')+F.col('Transaction_Value')))\\\n",
    "         .withColumn('Market_Price_Effect', ((F.col(Shares_Outstanding)-F.col('Net_Transaction'))*(F.col(Price)-F.col('Price_Lag'))+(F.col('Net_Transaction')*(F.col(Price)-F.col('Mean_price')))-F.col('Market_FX_Effect')))\\\n",
    "         .withColumn('Other_Changes_Volumne',(F.col('Balance')-F.col('Balance_Lag')-F.col('Market_Price_Effect')-F.col('Market_FX_Effect')-F.col('Transactions_Value')))\\\n",
    "         .withColumn('pct_OVC', (((F.col('Balance')-F.col('Balance_Lag')-F.col('Market_Price_Effect')-F.col('Market_FX_Effect')-F.col('Transactions_Value'))/(F.col('Balance')))*100))\\\n",
    "         .drop('Shares_Outstanding_Lag',\n",
    "               'Price_Lag',\n",
    "               'Exchange_Rate_Lag',\n",
    "               'Delta_Price',\n",
    "               'Delta_Exchange_rate',\n",
    "               'Mean_price',\n",
    "               'Mean_Exchange_rate',\n",
    "               'Balance_Lag'\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a tabular version of ROCKY V for every security can be of some vlaue, the vast size of the data makes this impractical for visualisation. For this reason data is aggregated to the sector level before visualisation. All variables arre conducive to simply being summed, hovever sum of price and sum of pct_OVC is meaningless, and hus the mean has been chosen instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4 = df3.filter(F.col(Quarter)>= Data_Start)\\\n",
    "         .groupBy([F.col(Quarter),F.col(Sector)])\\\n",
    "         .agg(F.sum(F.col(Shares_Outstanding)).alias('Shares_Outstanding'),\n",
    "              F.sum(F.col('Net_Transaction')).alias('Net_Transaction'),\n",
    "              F.sum(F.col('Balance')).alias('Balance'),\n",
    "              F.mean(F.col(Price)).alias('Price'),\n",
    "              F.sum(F.col('Transaction_Value')).alias('Transaction_Value'),\n",
    "              F.sum(F.col('Market_FX_Effect')).alias('Market_FX_Effect'),\n",
    "              F.sum(F.col('Market_Price_Effect')).alias('Market_Price_Effect'),\n",
    "              F.sum(F.col('Other_Changes_Volumne')).alias('Other_Changes_Volumne'),\n",
    "              F.mean(F.col('pct_OVC')).alias('pct_OVC')\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the by ID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3.orderBy(F.col(ID), F.col(Quarter)).repartition(1).write.csv.(Output+FA_Instrument_Select+'By_ID_Roky_V.csv', header = 'true', mode = 'Overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the by Sector dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4.orderBy(F.col(Quarter)).repartition(1).write.csv.(Output+FA_Instrument_Select+'By_Sector_Roky_V.csv', header = 'true', mode = 'Overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfere to Pandas for data visualisation\n",
    "Spark has no internal visualisation system, as such it needs to be transfered to pandas for this task.\n",
    "It is important to note that data should only be transfered at the aggregated state, as transfereing to Pandas pulls the entire data into the driver memory (The number set at session initiation) and might crash your system if the data is to big. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_df = df4.toPandas()\\\n",
    "           .set_index('Quarter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualisation\n",
    "\n",
    "## Summary of values\n",
    "### Sector balances\n",
    "The graph shows the total value of the selected Financial instrument by sector in GBP over time. \n",
    "These values should correspond to the Sector GBP Mkt_cap values of other QA checks\n",
    "\n",
    "Potential sources for error:\n",
    "* Sectorisation\n",
    "* Price\n",
    "* FX_tables\n",
    "* Shares outstanding for raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "pd_df.groupby('Sector')['Balance'].plot(kind='line',figsize=(20,5))\n",
    "plt.legends()\n",
    "plt.title('Balance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction values\n",
    "This graph shows the value of transations at mean price of t1 and t2\n",
    "\n",
    "Potential sources of error:\n",
    "* Calculation\n",
    "* Shares_Outstanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "pd_df.groupby('Sector')['Transaction_Value'].plot(kind='line',figsize=(20,5))\n",
    "plt.legends()\n",
    "plt.title('Transaction Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average price\n",
    "The following graph gives the average price of units per sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(3)\n",
    "pd_df.groupby('Sector')['Price'].plot(kind='line',figsize=(20,5))\n",
    "plt.legends()\n",
    "plt.title('Mean Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market FX effect\n",
    "This graph shows the effect of currency exchange rate changes on the sector balance after controlling for changes in number of shares in circulation and fluctuations in price.\n",
    "\n",
    "Potential sources of error:\n",
    "* Calculation\n",
    "* Price data\n",
    "* FX rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "pd_df.groupby('Sector')['Market_FX_Effect'].plot(kind='line',figsize=(20,5))\n",
    "plt.legends()\n",
    "plt.title('Market FX Effect')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price effect\n",
    "This graph shows the effect of price changes on the sector balance after controlling for changes in number of shares in circulation and fluctuations in currency exchange rates.  \n",
    "\n",
    "Potential sources of error:\n",
    "* Calculation\n",
    "* Price data\n",
    "* FX rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "pd_df.groupby('Sector')['Market_Price_Effect'].plot(kind='line',figsize=(20,5))\n",
    "plt.legends()\n",
    "plt.title('Market Price Effect')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Volume Changes \n",
    "Thie graph shows the sector OVC over time. Change here should only be seen if i.e. one company changes sector without dispanding. \n",
    "\n",
    "Note: Due to the current nature of data form TR we expect OVC's to be ~0, however due to how Spark handles large integers and calculation with integers the value might not be == 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(6)\n",
    "pd_df.groupby('Sector')['Other_Changes_Volumne'].plot(kind='line',figsize=(20,5))\n",
    "plt.legends()\n",
    "plt.title('Other Changes Volumne')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage Other Volume Changes \n",
    "Thie graph shows the percentage of total value OVC make out  over time. Change here should only be seen if i.e. one company changes sector without dispanding. \n",
    "\n",
    "Note: Due to the current nature of data form TR we expect OVC's to be ~0, however due to how Spark handles large integers and calculation with integers the value might not be == 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(7)\n",
    "pd_df.groupby('Sector')['pct_OVC'].plot(kind='line',figsize=(20,5))\n",
    "plt.legends()\n",
    "plt.title('Other Changes Volumne by percentage')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
